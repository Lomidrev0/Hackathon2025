from fastapi import FastAPI, Query
import psycopg2
from psycopg2.extras import RealDictCursor
from langchain_ollama import OllamaEmbeddings, OllamaLLM
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
import re
import os
import pickle

# -------------------------------
# ‚öôÔ∏è 1. Konfigur√°cia DB
# -------------------------------
DB_SERVER = "localhost"
DB_USER = "receipts_user"
DB_PASSWORD = "mypassword"
DB_NAME = "receiptsdb"
PORT = 15432

# -------------------------------
# ‚öôÔ∏è 2. Inicializ√°cia aplik√°cie
# -------------------------------
app = FastAPI(title="Receipts Vector AI Backend (with SQL fallback + training)")

# -------------------------------
# üß† 3. Pripojenie k DB
# -------------------------------
def fetch_all_receipts():
    conn = psycopg2.connect(
        host=DB_SERVER,
        database=DB_NAME,
        user=DB_USER,
        password=DB_PASSWORD,
        port=PORT
    )
    cursor = conn.cursor(cursor_factory=RealDictCursor)
    cursor.execute("SELECT * FROM item;")
    records = cursor.fetchall()
    cursor.close()
    conn.close()
    return records

# -------------------------------
# üî¢ 4. Modely a ukladanie indexu
# -------------------------------
embeddings = OllamaEmbeddings(model="mxbai-embed-large")
llm = OllamaLLM(model="llama3:instruct")

VECTOR_DIR = "vector_index"
os.makedirs(VECTOR_DIR, exist_ok=True)
FAISS_PATH = os.path.join(VECTOR_DIR, "receipts_index.faiss")
META_PATH = os.path.join(VECTOR_DIR, "receipts_meta.pkl")

vector_store = None

# -------------------------------
# üöÄ 5. Endpoint: vektoriz√°cia a ulo≈æenie
# -------------------------------
@app.get("/ai/vectorize")
async def vectorize_db():
    global vector_store
    records = fetch_all_receipts()
    if not records:
        return {"message": "Datab√°za je pr√°zdna."}

    documents = []
    for r in records:
        text = " | ".join([f"{k}: {v}" for k, v in r.items()])
        documents.append(Document(page_content=text))

    # vytvor FAISS index a ulo≈æ ho
    vector_store = FAISS.from_documents(documents, embeddings)
    vector_store.save_local(FAISS_PATH)

    with open(META_PATH, "wb") as f:
        pickle.dump({"count": len(documents)}, f)

    return {"message": f"Naindexovan√Ωch {len(documents)} z√°znamov z datab√°zy."}


# -------------------------------
# üß¨ 6. Endpoint: tr√©ning (roz≈°√≠renie indexu)
# -------------------------------
@app.get("/ai/train")
async def train_vector_model():
    """
    Pokroƒçil√Ω tr√©ning ‚Äì rozdel√≠ texty z DB na men≈°ie k√∫sky a vytvor√≠ robustnej≈°√≠ FAISS index.
    Ide o formu "RAG tr√©ningu", nie o klasick√Ω fine-tuning modelu.
    """
    global vector_store
    records = fetch_all_receipts()
    if not records:
        return {"message": "Datab√°za je pr√°zdna."}

    documents = []
    for r in records:
        text = " | ".join([f"{k}: {v}" for k, v in r.items()])
        documents.append(Document(page_content=text))

    # rozdelenie textov na men≈°ie k√∫sky
    splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)
    split_docs = splitter.split_documents(documents)

    vector_store = FAISS.from_documents(split_docs, embeddings)
    vector_store.save_local(FAISS_PATH)

    with open(META_PATH, "wb") as f:
        pickle.dump({"chunks": len(split_docs)}, f)

    return {"message": f"Model natr√©novan√Ω s {len(split_docs)} ƒçasticami (RAG index)."}


# -------------------------------
# üß≠ 7. Endpoint: AI ot√°zky s SQL fallbackom
# -------------------------------
@app.get("/ai/ask")
async def ask_ai(prompt: str = Query(..., description="Ot√°zka pre AI")):
    global vector_store

    # üßÆ SQL fallback pre ot√°zky o s√∫ƒçtoch, priemeroch a celkov√Ωch sum√°ch
    if re.search(r"(koƒæko|sum|spolu|celkov√°|total|sumu|suma)", prompt.lower()):
        if re.search(r"(pivo|beer)", prompt.lower()):
            conn = psycopg2.connect(
                host=DB_SERVER, database=DB_NAME,
                user=DB_USER, password=DB_PASSWORD, port=PORT
            )
            cursor = conn.cursor()
            cursor.execute("""
                SELECT SUM(price) FROM item
                WHERE name ILIKE '%pivo%' OR ai_name_in_english_without_brand_and_quantity ILIKE '%beer%';
            """)
            total = cursor.fetchone()[0]
            cursor.close()
            conn.close()

            if total:
                return {"question": prompt, "answer": f"Spolu si minul {total:.2f} ‚Ç¨ na piv√° üç∫."}
            else:
                return {"question": prompt, "answer": "V datab√°ze som nena≈°iel ≈æiadne piv√°."}

    # üîÑ Naƒç√≠taj FAISS index z disku, ak e≈°te nie je v pam√§ti
    if vector_store is None:
        try:
            vector_store = FAISS.load_local(FAISS_PATH, embeddings, allow_dangerous_deserialization=True)
        except Exception:
            return {"error": "Vektorov√Ω index neexistuje. Najsk√¥r spusti /ai/vectorize alebo /ai/train."}

    # üîé Vyhƒæadaj najrelevantnej≈°ie riadky
    results = vector_store.similarity_search(prompt, k=20)
    if not results:
        return {"answer": "Nena≈°li sa ≈æiadne relevantn√© v√Ωsledky."}

    # üßπ Vyƒçisti kontext
    def clean_context(documents):
        cleaned = []
        for d in documents:
            text = re.sub(r'\s*\|\s*', ', ', d.page_content)
            cleaned.append(text)
        return "\n".join(cleaned)

    context = clean_context(results)

    # üß† Prompt pre LLaMA3
    full_prompt = f"""
    Si inteligentn√Ω analytick√Ω asistent. Na z√°klade √∫dajov z datab√°zy odpovedz na ot√°zku pou≈æ√≠vateƒæa.
    Ka≈æd√Ω riadok obsahuje:
    - n√°zov produktu (name)
    - cenu (price)
    - kateg√≥riu (ai_category)
    - znaƒçku (ai_brand)

    Tu s√∫ najrelevantnej≈°ie d√°ta:
    {context}

    Odpovedz v√Ωhradne na z√°klade √∫dajov vy≈°≈°ie.
    Buƒè presn√Ω, odpovedz maxim√°lne v 2 vet√°ch.
    Ot√°zka pou≈æ√≠vateƒæa: {prompt}
    """

    answer = llm.invoke(full_prompt)

    return {
        "question": prompt,
        "answer": answer.strip(),
        "context_used": len(results)
    }


# -------------------------------
# üè† 8. Root endpoint
# -------------------------------
@app.get("/")
async def root():
    return {"message": "Vector AI backend (LLaMA3 + SQL fallback + tr√©ning) be≈æ√≠ üöÄ"}
